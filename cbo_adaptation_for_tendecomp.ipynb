{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "alpha = 10000 # Varying parameter: Influence parameter\n",
    "lambda_ = 180 # Varying parameter: Drift parameter\n",
    "sigma = 10 # Varying parameter: Diffusion parameter\n",
    "dt = [0.01] # Varying parameter: Time step\n",
    "n_particles = [100, 150] # Varying parameter: Number of particles, number of initial random tensors\n",
    "T = 1.0 # Fixed parameter: Total time\n",
    "#n_iterations = int(T / dt) # Varying parameter: Number of iterations\n",
    "\n",
    "# Create itertools.product to generate all combinations of parameters\n",
    "# param_grid = list(itertools.product(dt, n_particles, alpha, lambda_, sigma))\n",
    "\n",
    "I, J, K = 3, 3, 3 # Fixed parameters: Dims of initial tensor\n",
    "rank = 2 # Fixed parameter: number of rank-1 tensors\n",
    "\n",
    "# Create a random tensor (change np.random.seed(--) in case of need)\n",
    "tensor = tl.tensor(np.random.random((I, J, K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.37454012, 0.95071431, 0.73199394],\n        [0.59865848, 0.15601864, 0.15599452],\n        [0.05808361, 0.86617615, 0.60111501]],\n\n       [[0.70807258, 0.02058449, 0.96990985],\n        [0.83244264, 0.21233911, 0.18182497],\n        [0.18340451, 0.30424224, 0.52475643]],\n\n       [[0.43194502, 0.29122914, 0.61185289],\n        [0.13949386, 0.29214465, 0.36636184],\n        [0.45606998, 0.78517596, 0.19967378]]])"
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n# example indexing:\\na1_1 = particles[0]['A'][:,0]\\nb1_1 = particles[0]['B'][:,0]\\nc1_1 = particles[0]['C'][:,0]\\n\\na2_1 = particles[0]['A'][:,1]\\nb2_1 = particles[0]['B'][:,1]\\nc2_1 = particles[0]['C'][:,1]\\n\\na3_1 = particles[0]['A'][:,2]\\nb3_1 = particles[0]['B'][:,2]\\nc3_1 = particles[0]['C'][:,2]\\n\\na4_1 = particles[0]['A'][:,3]\\nb4_1 = particles[0]['B'][:,3]\\nc4_1 = particles[0]['C'][:,3]\\n\\na5_1 = particles[0]['A'][:,4]\\nb5_1 = particles[0]['B'][:,4]\\nc5_1 = particles[0]['C'][:,4]\\n\\n# ----------------------------\\n\\na1_2 = particles[1]['A'][:,0]\\nb1_2 = particles[1]['B'][:,0]\\n\\n# and so on\""
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# particles[i], where i = [A_i, B_i, C_i] and A_i.shape = 10x5, B_i.shape = 12x5, C_i.shape = 14x5\n",
    "# a{r}_{i} = particles[i-1]['A'][:,r-1]\n",
    "# where r = rank from 1 to 5, i = particles from 1 to n_particles\n",
    "\n",
    "\"\"\"\n",
    "# example indexing:\n",
    "a1_1 = particles[0]['A'][:,0]\n",
    "b1_1 = particles[0]['B'][:,0]\n",
    "c1_1 = particles[0]['C'][:,0]\n",
    "\n",
    "a2_1 = particles[0]['A'][:,1]\n",
    "b2_1 = particles[0]['B'][:,1]\n",
    "c2_1 = particles[0]['C'][:,1]\n",
    "\n",
    "a3_1 = particles[0]['A'][:,2]\n",
    "b3_1 = particles[0]['B'][:,2]\n",
    "c3_1 = particles[0]['C'][:,2]\n",
    "\n",
    "a4_1 = particles[0]['A'][:,3]\n",
    "b4_1 = particles[0]['B'][:,3]\n",
    "c4_1 = particles[0]['C'][:,3]\n",
    "\n",
    "a5_1 = particles[0]['A'][:,4]\n",
    "b5_1 = particles[0]['B'][:,4]\n",
    "c5_1 = particles[0]['C'][:,4]\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "a1_2 = particles[1]['A'][:,0]\n",
    "b1_2 = particles[1]['B'][:,0]\n",
    "\n",
    "# and so on\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective_function(particle):\n",
    "    # Reconstruct tensor from the given particle\n",
    "    A = particle['A']\n",
    "    B = particle['B']\n",
    "    C = particle['C']\n",
    "    reconstructed_tensor = cp_to_tensor((np.ones(rank), [A, B, C]))\n",
    "\n",
    "    # Compute the Frobenius norm of the difference\n",
    "    error = tl.norm(tensor - reconstructed_tensor)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_consensus_point(particles, alpha):\n",
    "    # Find the particle with the minimum energy (x*)\n",
    "    min_energy_particle = min(particles, key=objective_function)\n",
    "    min_energy = objective_function(min_energy_particle)\n",
    "\n",
    "    # Initialize the numerators and the denominator\n",
    "    numerator_A = np.zeros_like(particles[0]['A'])\n",
    "    numerator_B = np.zeros_like(particles[0]['B'])\n",
    "    numerator_C = np.zeros_like(particles[0]['C'])\n",
    "    denominator = 0.0\n",
    "\n",
    "    for particle in particles:\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "\n",
    "        # Compute the energy for the current particle\n",
    "        energy = objective_function(particle)\n",
    "\n",
    "        # Apply the numerical stabilization trick\n",
    "        weight = np.exp(-alpha * (energy - min_energy))\n",
    "\n",
    "        # Update the numerators and the denominator\n",
    "        numerator_A += weight * A\n",
    "        numerator_B += weight * B\n",
    "        numerator_C += weight * C\n",
    "        denominator += weight\n",
    "\n",
    "    # Compute the consensus matrices (now no need for epsilon)\n",
    "    consensus_A = numerator_A / denominator\n",
    "    consensus_B = numerator_B / denominator\n",
    "    consensus_C = numerator_C / denominator\n",
    "\n",
    "    return {'A': consensus_A, 'B': consensus_B, 'C': consensus_C}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"def compute_consensus_point(particles, alpha):\\n    numerator_A = np.zeros_like(particles[0]['A'])\\n    numerator_B = np.zeros_like(particles[0]['B'])\\n    numerator_C = np.zeros_like(particles[0]['C'])\\n    denominator = 0.0\\n\\n    for particle in particles:\\n        A, B, C = particle['A'], particle['B'], particle['C']\\n\\n        # Compute the energy for the current particle (Frobenius norm error)\\n        energy = objective_function(particle)\\n\\n        # Compute the weight for this particle\\n        weight = np.exp(-alpha * energy)\\n\\n        # Update the numerators and the denominator\\n        numerator_A += weight * A\\n        numerator_B += weight * B\\n        numerator_C += weight * C\\n        denominator += weight\\n\\n    # Add a small epsilon to avoid division by zero\\n    epsilon = 1e-10\\n    denominator = np.maximum(denominator, epsilon)\\n\\n    # Compute the consensus matrices\\n    consensus_A = numerator_A / denominator\\n    consensus_B = numerator_B / denominator\\n    consensus_C = numerator_C / denominator\\n\\n    return {'A': consensus_A, 'B': consensus_B, 'C': consensus_C}\""
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def compute_consensus_point(particles, alpha):\n",
    "    numerator_A = np.zeros_like(particles[0]['A'])\n",
    "    numerator_B = np.zeros_like(particles[0]['B'])\n",
    "    numerator_C = np.zeros_like(particles[0]['C'])\n",
    "    denominator = 0.0\n",
    "\n",
    "    for particle in particles:\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "\n",
    "        # Compute the energy for the current particle (Frobenius norm error)\n",
    "        energy = objective_function(particle)\n",
    "\n",
    "        # Compute the weight for this particle\n",
    "        weight = np.exp(-alpha * energy)\n",
    "\n",
    "        # Update the numerators and the denominator\n",
    "        numerator_A += weight * A\n",
    "        numerator_B += weight * B\n",
    "        numerator_C += weight * C\n",
    "        denominator += weight\n",
    "\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    denominator = np.maximum(denominator, epsilon)\n",
    "\n",
    "    # Compute the consensus matrices\n",
    "    consensus_A = numerator_A / denominator\n",
    "    consensus_B = numerator_B / denominator\n",
    "    consensus_C = numerator_C / denominator\n",
    "\n",
    "    return {'A': consensus_A, 'B': consensus_B, 'C': consensus_C}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additional projection operator for the hypersurface and the special case hypersphere\n",
    "def projection_operator_matrix(V):\n",
    "    \"\"\"Apply the projection operator P(V) = I - (V ⊗ V) / |V|^2 to each column of the matrix V.\"\"\"\n",
    "    V_norm_sq = np.sum(V**2, axis=0, keepdims=True)  # Compute |V|^2 for each column (1 x n_columns)\n",
    "    outer_product = np.einsum('ik,jk->ijk', V, V)  # Compute V ⊗ V for each column (n_rows x n_rows x n_columns)\n",
    "    I = np.eye(V.shape[0])  # Identity matrix of appropriate size (n_rows x n_rows)\n",
    "    P = I[:, :, np.newaxis] - outer_product / V_norm_sq  # Apply the projection operator column-wise\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "outputs": [],
   "source": [
    "# Create random normal values between 0 and 1 for the Brownıan motion\n",
    "\n",
    "def truncated_normal_samples(size, mean=0, std=1, lower=0, upper=1):\n",
    "    # size can be a tuple like (3,4)\n",
    "    total_samples = np.prod(size)  # total number of elements\n",
    "    samples = np.empty(total_samples)\n",
    "    count = 0\n",
    "    while count < total_samples:\n",
    "        batch = np.random.normal(loc=mean, scale=std, size=total_samples - count)\n",
    "        batch = batch[(batch >= lower) & (batch <= upper)]\n",
    "        samples[count:count+len(batch)] = batch\n",
    "        count += len(batch)\n",
    "    # Reshape the samples to the desired shape\n",
    "    return samples.reshape(size)\n",
    "\n",
    "#arr_truncated = truncated_normal_samples(size=(3,4), mean=0, std=1, lower=0, upper=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isotropic_update(particles, consensus_point, lambda_, sigma, dt):\n",
    "    # Compute the consensus loss using the given consensus_point\n",
    "    consensus_point_loss = objective_function(consensus_point)\n",
    "\n",
    "    for particle in particles:\n",
    "        \"\"\"\n",
    "        Perform isotropic updates on all particles.\n",
    "\n",
    "        Parameters:\n",
    "        - particles: list of particle dictionaries with keys 'A', 'B', 'C'.\n",
    "        - consensus_point: dictionary with keys 'A', 'B', 'C' representing the consensus matrices.\n",
    "        - lambda_: drift parameter.\n",
    "        - sigma: diffusion parameter.\n",
    "        - dt: time step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the matrices A, B, C from the particle and the consensus\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "        A_consensus, B_consensus, C_consensus = consensus_point['A'], consensus_point['B'], consensus_point['C']\n",
    "\n",
    "        if consensus_point_loss < objective_function(particle):\n",
    "            drift_A = (-lambda_) * (A - A_consensus) * dt # I x r\n",
    "            drift_B = (-lambda_) * (B - B_consensus) * dt # J x r\n",
    "            drift_C = (-lambda_) * (C - C_consensus) * dt # K x r\n",
    "        else:\n",
    "            drift_A = np.zeros_like(A) * dt # I x r\n",
    "            drift_B = np.zeros_like(B) * dt # J x r\n",
    "            drift_C = np.zeros_like(C) * dt # K x r\n",
    "\n",
    "        # Create independent Gaussian noise for each element of A, B, C: Brownian motion\n",
    "        B_A = truncated_normal_samples(size=A.shape, mean=0, std=1, lower=0, upper=1)\n",
    "        B_B = truncated_normal_samples(size=B.shape, mean=0, std=1, lower=0, upper=1)\n",
    "        B_C = truncated_normal_samples(size=C.shape, mean=0, std=1, lower=0, upper=1)\n",
    "\n",
    "        # Isotropic noise term, applied column_wise\n",
    "        noise_A = sigma * np.linalg.norm(A - A_consensus) * B_A * np.sqrt(dt)\n",
    "        noise_B = sigma * np.linalg.norm(B - B_consensus) * B_B * np.sqrt(dt)\n",
    "        noise_C = sigma * np.linalg.norm(C - C_consensus) * B_C * np.sqrt(dt)\n",
    "\n",
    "        # Update particle's A, B, and C matrices\n",
    "        particle['A'] += np.array(drift_A) + np.array(noise_A)\n",
    "        particle['B'] += np.array(drift_B) + np.array(noise_B)\n",
    "        particle['C'] += np.array(drift_C) + np.array(noise_C)\n",
    "\n",
    "    return particles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def anisotropic_update(particles, consensus_point, lambda_, sigma, dt):\n",
    "    # Compute the consensus loss using the given consensus_point\n",
    "    consensus_point_loss = objective_function(consensus_point)\n",
    "\n",
    "    for particle in particles:\n",
    "        # Extract the matrices A, B, C from the particle and the consensus\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "        A_consensus, B_consensus, C_consensus = consensus_point['A'], consensus_point['B'], consensus_point['C']\n",
    "\n",
    "        if consensus_point_loss < objective_function(particle):\n",
    "            drift_A = (-lambda_) * (A - A_consensus) * dt # I x r\n",
    "            drift_B = (-lambda_) * (B - B_consensus) * dt # J x r\n",
    "            drift_C = (-lambda_) * (C - C_consensus) * dt # K x r\n",
    "        else:\n",
    "            drift_A = np.zeros_like(A) * dt # I x r\n",
    "            drift_B = np.zeros_like(B) * dt # J x r\n",
    "            drift_C = np.zeros_like(C) * dt # K x r\n",
    "\n",
    "        # Create independent Gaussian noise for each element of A, B, C: Brownian motion\n",
    "        B_A = truncated_normal_samples(size=A.shape, mean=0, std=1, lower=0, upper=1)\n",
    "        B_B = truncated_normal_samples(size=B.shape, mean=0, std=1, lower=0, upper=1)\n",
    "        B_C = truncated_normal_samples(size=C.shape, mean=0, std=1, lower=0, upper=1)\n",
    "\n",
    "        # Anisotropic diffusion terms\n",
    "        diffusion_A = sigma * (A - A_consensus) * B_A * np.sqrt(dt)\n",
    "        diffusion_B = sigma * (B - B_consensus) * B_B * np.sqrt(dt)\n",
    "        diffusion_C = sigma * (C - C_consensus) * B_C * np.sqrt(dt)\n",
    "\n",
    "        # Update particle's A, B, and C matrices\n",
    "        particle['A'] += drift_A + diffusion_A\n",
    "        particle['B'] += drift_B + diffusion_B\n",
    "        particle['C'] += drift_C + diffusion_C\n",
    "\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [],
   "source": [
    "particles = []\n",
    "nu=1000\n",
    "n_iterations=100\n",
    "for _ in range(nu):\n",
    "    A = np.random.randn(I, rank) # n_particles Ixrank matrices\n",
    "    B = np.random.randn(J, rank) # n_particles Jxrank matrices\n",
    "    C = np.random.randn(K, rank) # n_particles Kxrank matrices\n",
    "    particles.append({'A': A, 'B': B, 'C': C})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, Consensus Reconstruction Error: 2.648223233871237\n",
      "Iteration 2/100, Consensus Reconstruction Error: 1.9524582617252402\n",
      "Iteration 3/100, Consensus Reconstruction Error: 1.7076532271240368\n",
      "Iteration 4/100, Consensus Reconstruction Error: 1.4845644324511644\n",
      "Iteration 5/100, Consensus Reconstruction Error: 1.3317847158715823\n",
      "Iteration 6/100, Consensus Reconstruction Error: 1.2558100548196232\n",
      "Iteration 7/100, Consensus Reconstruction Error: 1.215804659662353\n",
      "Iteration 8/100, Consensus Reconstruction Error: 1.1887013027415072\n",
      "Iteration 9/100, Consensus Reconstruction Error: 1.1753947385662278\n",
      "Iteration 10/100, Consensus Reconstruction Error: 1.167880356357271\n",
      "Iteration 11/100, Consensus Reconstruction Error: 1.1642397002468834\n",
      "Iteration 12/100, Consensus Reconstruction Error: 1.1617327452236874\n",
      "Iteration 13/100, Consensus Reconstruction Error: 1.160315928620487\n",
      "Iteration 14/100, Consensus Reconstruction Error: 1.159520044419672\n",
      "Iteration 15/100, Consensus Reconstruction Error: 1.1591353532281776\n",
      "Iteration 16/100, Consensus Reconstruction Error: 1.1589884270193391\n",
      "Iteration 17/100, Consensus Reconstruction Error: 1.1589116577711047\n",
      "Iteration 18/100, Consensus Reconstruction Error: 1.1588392778528993\n",
      "Iteration 19/100, Consensus Reconstruction Error: 1.1586877431199178\n",
      "Iteration 20/100, Consensus Reconstruction Error: 1.158368844325649\n",
      "Iteration 21/100, Consensus Reconstruction Error: 1.1581699724725911\n",
      "Iteration 22/100, Consensus Reconstruction Error: 1.1568382751503945\n",
      "Iteration 23/100, Consensus Reconstruction Error: 1.1559034237817776\n",
      "Iteration 24/100, Consensus Reconstruction Error: 1.1552538902642293\n",
      "Iteration 25/100, Consensus Reconstruction Error: 1.1548582264134792\n",
      "Iteration 26/100, Consensus Reconstruction Error: 1.1546531050366127\n",
      "Iteration 27/100, Consensus Reconstruction Error: 1.1545571176205611\n",
      "Iteration 28/100, Consensus Reconstruction Error: 1.1544931363762363\n",
      "Iteration 29/100, Consensus Reconstruction Error: 1.1543887642939608\n",
      "Iteration 30/100, Consensus Reconstruction Error: 1.1540489544085781\n",
      "Iteration 31/100, Consensus Reconstruction Error: 1.1537889102905274\n",
      "Iteration 32/100, Consensus Reconstruction Error: 1.1535093608929627\n",
      "Iteration 33/100, Consensus Reconstruction Error: 1.1530940080899874\n",
      "Iteration 34/100, Consensus Reconstruction Error: 1.152869271479067\n",
      "Iteration 35/100, Consensus Reconstruction Error: 1.15255447785248\n",
      "Iteration 36/100, Consensus Reconstruction Error: 1.1522948806904283\n",
      "Iteration 37/100, Consensus Reconstruction Error: 1.152188278426353\n",
      "Iteration 38/100, Consensus Reconstruction Error: 1.1521324299467597\n",
      "Iteration 39/100, Consensus Reconstruction Error: 1.1519839491442447\n",
      "Iteration 40/100, Consensus Reconstruction Error: 1.151847952939094\n",
      "Iteration 41/100, Consensus Reconstruction Error: 1.1517673783124591\n",
      "Iteration 42/100, Consensus Reconstruction Error: 1.1516581978016918\n",
      "Iteration 43/100, Consensus Reconstruction Error: 1.1513845703227952\n",
      "Iteration 44/100, Consensus Reconstruction Error: 1.1510035606000077\n",
      "Iteration 45/100, Consensus Reconstruction Error: 1.1508205920619166\n",
      "Iteration 46/100, Consensus Reconstruction Error: 1.1507309423200698\n",
      "Iteration 47/100, Consensus Reconstruction Error: 1.1503822795167955\n",
      "Iteration 48/100, Consensus Reconstruction Error: 1.1501873383867096\n",
      "Iteration 49/100, Consensus Reconstruction Error: 1.150010004035576\n",
      "Iteration 50/100, Consensus Reconstruction Error: 1.149868739880096\n",
      "Iteration 51/100, Consensus Reconstruction Error: 1.1492901740622319\n",
      "Iteration 52/100, Consensus Reconstruction Error: 1.1489542950862937\n",
      "Iteration 53/100, Consensus Reconstruction Error: 1.1488169260460706\n",
      "Iteration 54/100, Consensus Reconstruction Error: 1.1487423662939056\n",
      "Iteration 55/100, Consensus Reconstruction Error: 1.1486343261748209\n",
      "Iteration 56/100, Consensus Reconstruction Error: 1.1485434822875877\n",
      "Iteration 57/100, Consensus Reconstruction Error: 1.148129114686381\n",
      "Iteration 58/100, Consensus Reconstruction Error: 1.1478912022025949\n",
      "Iteration 59/100, Consensus Reconstruction Error: 1.1478024791966022\n",
      "Iteration 60/100, Consensus Reconstruction Error: 1.1477446817209658\n",
      "Iteration 61/100, Consensus Reconstruction Error: 1.1477001515514615\n",
      "Iteration 62/100, Consensus Reconstruction Error: 1.1476540441456597\n",
      "Iteration 63/100, Consensus Reconstruction Error: 1.1474664791092954\n",
      "Iteration 64/100, Consensus Reconstruction Error: 1.1470666271658598\n",
      "Iteration 65/100, Consensus Reconstruction Error: 1.14686251838988\n",
      "Iteration 66/100, Consensus Reconstruction Error: 1.1467818556189706\n",
      "Iteration 67/100, Consensus Reconstruction Error: 1.1467356662449109\n",
      "Iteration 68/100, Consensus Reconstruction Error: 1.1466879289106453\n",
      "Iteration 69/100, Consensus Reconstruction Error: 1.146616197746385\n",
      "Iteration 70/100, Consensus Reconstruction Error: 1.1464353950141715\n",
      "Iteration 71/100, Consensus Reconstruction Error: 1.1460865874174642\n",
      "Iteration 72/100, Consensus Reconstruction Error: 1.145839821052627\n",
      "Iteration 73/100, Consensus Reconstruction Error: 1.145644850552329\n",
      "Iteration 74/100, Consensus Reconstruction Error: 1.145540032426286\n",
      "Iteration 75/100, Consensus Reconstruction Error: 1.1451825208796151\n",
      "Iteration 76/100, Consensus Reconstruction Error: 1.1449649134598066\n",
      "Iteration 77/100, Consensus Reconstruction Error: 1.1446013850724797\n",
      "Iteration 78/100, Consensus Reconstruction Error: 1.1444140037585377\n",
      "Iteration 79/100, Consensus Reconstruction Error: 1.1443438871145466\n",
      "Iteration 80/100, Consensus Reconstruction Error: 1.1442995074506186\n",
      "Iteration 81/100, Consensus Reconstruction Error: 1.1442593228603928\n",
      "Iteration 82/100, Consensus Reconstruction Error: 1.144211253564991\n",
      "Iteration 83/100, Consensus Reconstruction Error: 1.1440311513570467\n",
      "Iteration 84/100, Consensus Reconstruction Error: 1.1439184957723973\n",
      "Iteration 85/100, Consensus Reconstruction Error: 1.1437681580603574\n",
      "Iteration 86/100, Consensus Reconstruction Error: 1.143684632166373\n",
      "Iteration 87/100, Consensus Reconstruction Error: 1.1435357342229082\n",
      "Iteration 88/100, Consensus Reconstruction Error: 1.1433488885089833\n",
      "Iteration 89/100, Consensus Reconstruction Error: 1.1431740778763249\n",
      "Iteration 90/100, Consensus Reconstruction Error: 1.1429097233612524\n",
      "Iteration 91/100, Consensus Reconstruction Error: 1.1427156833459788\n",
      "Iteration 92/100, Consensus Reconstruction Error: 1.1421162458958005\n",
      "Iteration 93/100, Consensus Reconstruction Error: 1.1417577956197391\n",
      "Iteration 94/100, Consensus Reconstruction Error: 1.141596385986929\n",
      "Iteration 95/100, Consensus Reconstruction Error: 1.1415102454516919\n",
      "Iteration 96/100, Consensus Reconstruction Error: 1.1414412605317652\n",
      "Iteration 97/100, Consensus Reconstruction Error: 1.1413234478774812\n",
      "Iteration 98/100, Consensus Reconstruction Error: 1.1410962880606867\n",
      "Iteration 99/100, Consensus Reconstruction Error: 1.140952727052558\n",
      "Iteration 100/100, Consensus Reconstruction Error: 1.1407748600227992\n"
     ]
    }
   ],
   "source": [
    "# Main iteration loop\n",
    "for iteration in range(n_iterations):\n",
    "    # Step 1: Calculate the consensus point\n",
    "    consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "\n",
    "    # Step 2: Compute the reconstruction error for the consensus point\n",
    "    consensus_tensor = cp_to_tensor((np.ones(rank), [consensus_point['A'], consensus_point['B'], consensus_point['C']]))\n",
    "    consensus_error = tl.norm(tensor - consensus_tensor)\n",
    "\n",
    "    # Print the reconstruction error of the consensus point\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations}, Consensus Reconstruction Error: {consensus_error}\")\n",
    "\n",
    "    # Step 3: Update the particles (either isotropic or anisotropic)\n",
    "    particles = anisotropic_update(particles, consensus_point, lambda_, sigma, dt)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'results = []\\n# Create itertools.product to generate all combinations of parameters\\nparam_grid = list(itertools.product(dt, n_particles, alpha, lambda_, sigma))\\n\\n# Iterate over the parameter grid\\nfor params in param_grid:\\n    dt, n_particles, alpha, lambda_, sigma = params\\n\\n    n_iterations = int(T / dt) # Varying parameter: Number of iterations\\n\\n    # Step 2: Initialize\\n    particles = []\\n    for _ in range(n_particles):\\n        A = np.random.randn(I, rank) # n_particles Ixrank matrices\\n        B = np.random.randn(J, rank) # n_particles Jxrank matrices\\n        C = np.random.randn(K, rank) # n_particles Kxrank matrices\\n        particles.append({\\'A\\': A, \\'B\\': B, \\'C\\': C})\\n\\n    # Print all parameters including fixed ones\\n    print(f\"Running grid search with params: dt={dt}, n_particles={n_particles}, alpha={alpha}, lambda_={lambda_}, sigma={sigma}, T={T}, I={I}, J={J}, K={K}, rank={rank}, #iters={n_iterations}\")\\n\\n    # List to store reconstruction errors of the consensus point at each iteration\\n    consensus_errors = []\\n\\n    # Main iteration loop\\n    for iteration in range(n_iterations):\\n        # Step 1: Calculate the consensus point\\n        consensus_point = compute_consensus_point(particles, alpha=alpha)\\n\\n        # Step 2: Compute the reconstruction error for the consensus point\\n        consensus_tensor = cp_to_tensor((np.ones(rank), [consensus_point[\\'A\\'], consensus_point[\\'B\\'], consensus_point[\\'C\\']]))\\n        consensus_error = tl.norm(tensor - consensus_tensor)\\n\\n        # Print the reconstruction error of the consensus point\\n        print(f\"Iteration {iteration + 1}/{n_iterations}, Consensus Reconstruction Error: {consensus_error}\")\\n\\n        # Store the error for later plotting\\n        consensus_errors.append(consensus_error)\\n\\n        # Step 3: Update the particles (either isotropic or anisotropic)\\n        anisotropic_update(particles, consensus_point, lambda_, sigma)\\n\\n    # Collect errors at specific iterations: 1st, 1/4th, 1/2th, 3/4th, and last iteration\\n    error_begin = consensus_errors[0]\\n    error_iter_1_4 = consensus_errors[int(n_iterations * 0.25)]\\n    error_iter_1_2 = consensus_errors[int(n_iterations * 0.5)]\\n    error_iter_3_4 = consensus_errors[int(n_iterations * 0.75)]\\n    error_last = consensus_errors[-1]\\n\\n    # Add the parameters and errors to the results list\\n    results.append({\\n        \\'alpha\\': alpha,\\n        \\'lambda\\': lambda_,\\n        \\'sigma\\': sigma,\\n        \\'T\\': T,\\n        \\'dt\\': dt,\\n        \\'#iters (T/dt)\\': n_iterations,\\n        \\'#particles\\': n_particles,\\n        \\'I\\': I,\\n        \\'J\\': J,\\n        \\'K\\': K,\\n        \\'rank\\': rank,\\n        \\'error_begin\\': error_begin,\\n        \\'error_iter 1/4\\': error_iter_1_4,\\n        \\'error_iter 1/2\\': error_iter_1_2,\\n        \\'error_iter 3/4\\': error_iter_3_4,\\n        \\'error_iter last\\': error_last,\\n        \\'landscape\\': \\'\\'  # Fill this field if needed\\n    })\\n\\n    # Final output: Save or analyze the final consensus point\\n    final_consensus_point = compute_consensus_point(particles, alpha=alpha)\\n    final_consensus_tensor = cp_to_tensor((np.ones(rank), [final_consensus_point[\\'A\\'], final_consensus_point[\\'B\\'], final_consensus_point[\\'C\\']]))\\n    final_error = tl.norm(tensor - final_consensus_tensor)\\n\\n    print(\"Final Consensus Reconstruction Error: \", final_error)\\n\\n    # Plotting the consensus reconstruction errors over iterations\\n    plt.plot(consensus_errors, label=f\\'Consensus Error (dt={dt}, np={n_particles}, α={alpha}, λ={lambda_}, σ={sigma})\\')\\n\\n# Final plot formatting (if multiple runs are plotted)\\nplt.xlabel(\\'Iteration\\')\\nplt.ylabel(\\'Reconstruction Error\\')\\nplt.title(\\'Reconstruction Error of Consensus Point Over Iterations (Grid Search)\\')\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\n\\n# Convert results to a pandas DataFrame\\ndf_results = pd.DataFrame(results)'"
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"results = []\n",
    "# Create itertools.product to generate all combinations of parameters\n",
    "param_grid = list(itertools.product(dt, n_particles, alpha, lambda_, sigma))\n",
    "\n",
    "# Iterate over the parameter grid\n",
    "for params in param_grid:\n",
    "    dt, n_particles, alpha, lambda_, sigma = params\n",
    "\n",
    "    n_iterations = int(T / dt) # Varying parameter: Number of iterations\n",
    "\n",
    "    # Step 2: Initialize\n",
    "    particles = []\n",
    "    for _ in range(n_particles):\n",
    "        A = np.random.randn(I, rank) # n_particles Ixrank matrices\n",
    "        B = np.random.randn(J, rank) # n_particles Jxrank matrices\n",
    "        C = np.random.randn(K, rank) # n_particles Kxrank matrices\n",
    "        particles.append({'A': A, 'B': B, 'C': C})\n",
    "\n",
    "    # Print all parameters including fixed ones\n",
    "    print(f\"Running grid search with params: dt={dt}, n_particles={n_particles}, alpha={alpha}, lambda_={lambda_}, sigma={sigma}, T={T}, I={I}, J={J}, K={K}, rank={rank}, #iters={n_iterations}\")\n",
    "\n",
    "    # List to store reconstruction errors of the consensus point at each iteration\n",
    "    consensus_errors = []\n",
    "\n",
    "    # Main iteration loop\n",
    "    for iteration in range(n_iterations):\n",
    "        # Step 1: Calculate the consensus point\n",
    "        consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "\n",
    "        # Step 2: Compute the reconstruction error for the consensus point\n",
    "        consensus_tensor = cp_to_tensor((np.ones(rank), [consensus_point['A'], consensus_point['B'], consensus_point['C']]))\n",
    "        consensus_error = tl.norm(tensor - consensus_tensor)\n",
    "\n",
    "        # Print the reconstruction error of the consensus point\n",
    "        print(f\"Iteration {iteration + 1}/{n_iterations}, Consensus Reconstruction Error: {consensus_error}\")\n",
    "\n",
    "        # Store the error for later plotting\n",
    "        consensus_errors.append(consensus_error)\n",
    "\n",
    "        # Step 3: Update the particles (either isotropic or anisotropic)\n",
    "        anisotropic_update(particles, consensus_point, lambda_, sigma)\n",
    "\n",
    "    # Collect errors at specific iterations: 1st, 1/4th, 1/2th, 3/4th, and last iteration\n",
    "    error_begin = consensus_errors[0]\n",
    "    error_iter_1_4 = consensus_errors[int(n_iterations * 0.25)]\n",
    "    error_iter_1_2 = consensus_errors[int(n_iterations * 0.5)]\n",
    "    error_iter_3_4 = consensus_errors[int(n_iterations * 0.75)]\n",
    "    error_last = consensus_errors[-1]\n",
    "\n",
    "    # Add the parameters and errors to the results list\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'lambda': lambda_,\n",
    "        'sigma': sigma,\n",
    "        'T': T,\n",
    "        'dt': dt,\n",
    "        '#iters (T/dt)': n_iterations,\n",
    "        '#particles': n_particles,\n",
    "        'I': I,\n",
    "        'J': J,\n",
    "        'K': K,\n",
    "        'rank': rank,\n",
    "        'error_begin': error_begin,\n",
    "        'error_iter 1/4': error_iter_1_4,\n",
    "        'error_iter 1/2': error_iter_1_2,\n",
    "        'error_iter 3/4': error_iter_3_4,\n",
    "        'error_iter last': error_last,\n",
    "        'landscape': ''  # Fill this field if needed\n",
    "    })\n",
    "\n",
    "    # Final output: Save or analyze the final consensus point\n",
    "    final_consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "    final_consensus_tensor = cp_to_tensor((np.ones(rank), [final_consensus_point['A'], final_consensus_point['B'], final_consensus_point['C']]))\n",
    "    final_error = tl.norm(tensor - final_consensus_tensor)\n",
    "\n",
    "    print(\"Final Consensus Reconstruction Error: \", final_error)\n",
    "\n",
    "    # Plotting the consensus reconstruction errors over iterations\n",
    "    plt.plot(consensus_errors, label=f'Consensus Error (dt={dt}, np={n_particles}, α={alpha}, λ={lambda_}, σ={sigma})')\n",
    "\n",
    "# Final plot formatting (if multiple runs are plotted)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error of Consensus Point Over Iterations (Grid Search)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "df_results = pd.DataFrame(results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
