{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "alpha = [100, 1000, 10000, 100000] # Varying parameter: Influence parameter\n",
    "lambda_ = [1000, 10000] # Varying parameter: Drift parameter\n",
    "sigma = [50, 80] # Varying parameter: Diffusion parameter\n",
    "dt = [0.01] # Varying parameter: Time step\n",
    "n_particles = [100, 150] # Varying parameter: Number of particles, number of initial random tensors\n",
    "T = 1.0 # Fixed parameter: Total time\n",
    "#n_iterations = int(T / dt) # Varying parameter: Number of iterations\n",
    "\n",
    "# Create itertools.product to generate all combinations of parameters\n",
    "param_grid = list(itertools.product(dt, n_particles, alpha, lambda_, sigma))\n",
    "\n",
    "I, J, K = 3, 3, 3 # Fixed parameters: Dims of initial tensor\n",
    "rank = 2 # Fixed parameter: number of rank-1 tensors\n",
    "\n",
    "# Create a random tensor (change np.random.seed(--) in case of need)\n",
    "tensor = tl.tensor(np.random.random((I, J, K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# particles[i], where i = [A_i, B_i, C_i] and A_i.shape = 10x5, B_i.shape = 12x5, C_i.shape = 14x5\n",
    "# a{r}_{i} = particles[i-1]['A'][:,r-1]\n",
    "# where r = rank from 1 to 5, i = particles from 1 to n_particles\n",
    "\n",
    "\"\"\"\n",
    "# example indexing:\n",
    "a1_1 = particles[0]['A'][:,0]\n",
    "b1_1 = particles[0]['B'][:,0]\n",
    "c1_1 = particles[0]['C'][:,0]\n",
    "\n",
    "a2_1 = particles[0]['A'][:,1]\n",
    "b2_1 = particles[0]['B'][:,1]\n",
    "c2_1 = particles[0]['C'][:,1]\n",
    "\n",
    "a3_1 = particles[0]['A'][:,2]\n",
    "b3_1 = particles[0]['B'][:,2]\n",
    "c3_1 = particles[0]['C'][:,2]\n",
    "\n",
    "a4_1 = particles[0]['A'][:,3]\n",
    "b4_1 = particles[0]['B'][:,3]\n",
    "c4_1 = particles[0]['C'][:,3]\n",
    "\n",
    "a5_1 = particles[0]['A'][:,4]\n",
    "b5_1 = particles[0]['B'][:,4]\n",
    "c5_1 = particles[0]['C'][:,4]\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "a1_2 = particles[1]['A'][:,0]\n",
    "b1_2 = particles[1]['B'][:,0]\n",
    "\n",
    "# and so on\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective_function(particle):\n",
    "    # Reconstruct tensor from the given particle\n",
    "    A = particle['A']\n",
    "    B = particle['B']\n",
    "    C = particle['C']\n",
    "    reconstructed_tensor = cp_to_tensor((np.ones(rank), [A, B, C]))\n",
    "\n",
    "    # Compute the Frobenius norm of the difference\n",
    "    error = tl.norm(tensor - reconstructed_tensor)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_consensus_point(particles, alpha):\n",
    "    # Find the particle with the minimum energy (V* in the image)\n",
    "    min_energy_particle = min(particles, key=objective_function)\n",
    "    min_energy = objective_function(min_energy_particle)\n",
    "\n",
    "    # Initialize the numerators and the denominator\n",
    "    numerator_A = np.zeros_like(particles[0]['A'])\n",
    "    numerator_B = np.zeros_like(particles[0]['B'])\n",
    "    numerator_C = np.zeros_like(particles[0]['C'])\n",
    "    denominator = 0.0\n",
    "\n",
    "    for particle in particles:\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "\n",
    "        # Compute the energy for the current particle\n",
    "        energy = objective_function(particle)\n",
    "\n",
    "        # Apply the numerical stabilization trick\n",
    "        weight = np.exp(-alpha * (energy - min_energy))\n",
    "\n",
    "        # Update the numerators and the denominator\n",
    "        numerator_A += weight * A\n",
    "        numerator_B += weight * B\n",
    "        numerator_C += weight * C\n",
    "        denominator += weight\n",
    "\n",
    "    # Compute the consensus matrices (now no need for epsilon)\n",
    "    consensus_A = numerator_A / denominator\n",
    "    consensus_B = numerator_B / denominator\n",
    "    consensus_C = numerator_C / denominator\n",
    "\n",
    "    return {'A': consensus_A, 'B': consensus_B, 'C': consensus_C}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def compute_consensus_point(particles, alpha):\n",
    "    numerator_A = np.zeros_like(particles[0]['A'])\n",
    "    numerator_B = np.zeros_like(particles[0]['B'])\n",
    "    numerator_C = np.zeros_like(particles[0]['C'])\n",
    "    denominator = 0.0\n",
    "\n",
    "    for particle in particles:\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "\n",
    "        # Compute the energy for the current particle (Frobenius norm error)\n",
    "        energy = objective_function(particle)\n",
    "\n",
    "        # Compute the weight for this particle\n",
    "        weight = np.exp(-alpha * energy)\n",
    "\n",
    "        # Update the numerators and the denominator\n",
    "        numerator_A += weight * A\n",
    "        numerator_B += weight * B\n",
    "        numerator_C += weight * C\n",
    "        denominator += weight\n",
    "\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    denominator = np.maximum(denominator, epsilon)\n",
    "\n",
    "    # Compute the consensus matrices\n",
    "    consensus_A = numerator_A / denominator\n",
    "    consensus_B = numerator_B / denominator\n",
    "    consensus_C = numerator_C / denominator\n",
    "\n",
    "    return {'A': consensus_A, 'B': consensus_B, 'C': consensus_C}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projection_operator_matrix(V):\n",
    "    \"\"\"Apply the projection operator P(V) = I - (V ⊗ V) / |V|^2 to each column of the matrix V.\"\"\"\n",
    "    V_norm_sq = np.sum(V**2, axis=0, keepdims=True)  # Compute |V|^2 for each column (1 x n_columns)\n",
    "    outer_product = np.einsum('ik,jk->ijk', V, V)  # Compute V ⊗ V for each column (n_rows x n_rows x n_columns)\n",
    "    I = np.eye(V.shape[0])  # Identity matrix of appropriate size (n_rows x n_rows)\n",
    "    P = I[:, :, np.newaxis] - outer_product / V_norm_sq  # Apply the projection operator column-wise\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isotropic_update(particles, consensus_point, lambda_, sigma, dt):\n",
    "    for particle in particles:\n",
    "        \"\"\"\n",
    "        Perform isotropic updates on all particles.\n",
    "\n",
    "        Parameters:\n",
    "        - particles: list of particle dictionaries with keys 'A', 'B', 'C'.\n",
    "        - consensus_point: dictionary with keys 'A', 'B', 'C' representing the consensus matrices.\n",
    "        - lambda_: drift parameter.\n",
    "        - sigma: diffusion parameter.\n",
    "        - dt: time step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the matrices A, B, C from the particle and the consensus\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "        A_consensus, B_consensus, C_consensus = consensus_point['A'], consensus_point['B'], consensus_point['C']\n",
    "\n",
    "        if objective_function(compute_consensus_point(particles, alpha)) < objective_function(particle):\n",
    "            drift_A = (-lambda_) * (A - A_consensus) * dt\n",
    "            drift_B = (-lambda_) * (B - B_consensus) * dt\n",
    "            drift_C = (-lambda_) * (C - C_consensus) * dt\n",
    "        else:\n",
    "            drift_A = np.zeros(A.shape).tolist()\n",
    "            drift_B = np.zeros(B.shape).tolist()\n",
    "            drift_C = np.zeros(C.shape).tolist()\n",
    "\n",
    "        # Isotropic noise term, applied column_wise\n",
    "        noise_A = sigma * np.abs(A - A_consensus) * np.random.randn(*A.shape) * dt\n",
    "        noise_B = sigma * np.abs(B - B_consensus) * np.random.randn(*B.shape) * dt\n",
    "        noise_C = sigma * np.abs(C - C_consensus) * np.random.randn(*C.shape) * dt\n",
    "\n",
    "        # Update particle's A, B, and C matrices\n",
    "        particle['A'] += np.array(drift_A) + np.array(noise_A)\n",
    "        particle['B'] += np.array(drift_B) + np.array(noise_B)\n",
    "        particle['C'] += np.array(drift_C) + np.array(noise_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def anisotropic_update(particles, consensus_point, lambda_, sigma, dt):\n",
    "    for particle in particles:\n",
    "        # Extract the matrices A, B, C from the particle and the consensus\n",
    "        A, B, C = particle['A'], particle['B'], particle['C']\n",
    "        A_consensus, B_consensus, C_consensus = consensus_point['A'], consensus_point['B'], consensus_point['C']\n",
    "\n",
    "        # Compute the elementwise differences\n",
    "        diff_A = A - A_consensus\n",
    "        diff_B = B - B_consensus\n",
    "        diff_C = C - C_consensus\n",
    "\n",
    "        # Compute the projection terms for each matrix\n",
    "        P_A = projection_operator_matrix(A)\n",
    "        P_B = projection_operator_matrix(B)\n",
    "        P_C = projection_operator_matrix(C)\n",
    "\n",
    "        # Generate Brownian motion term\n",
    "        delta_B_A = np.random.randn(*A.shape) * np.sqrt(dt)\n",
    "        delta_B_B = np.random.randn(*B.shape) * np.sqrt(dt)\n",
    "        delta_B_C = np.random.randn(*C.shape) * np.sqrt(dt)\n",
    "\n",
    "        # Corrected drift term: includes projection P(V_ti) applied column-wis\n",
    "        drift_A = lambda_ * dt * np.einsum('ijk,jk->ik', P_A, A_consensus)\n",
    "        drift_B = lambda_ * dt * np.einsum('ijk,jk->ik', P_B, B_consensus)\n",
    "        drift_C = lambda_ * dt * np.einsum('ijk,jk->ik', P_C, C_consensus)\n",
    "\n",
    "        # Anisotropic noise term, projected and applied column-wise\n",
    "        D_A = np.diag(np.linalg.norm(diff_A, axis=0)**2)\n",
    "        noise_A = sigma * np.matmul(diff_A, D_A) * delta_B_A\n",
    "\n",
    "        D_B = np.diag(np.linalg.norm(diff_B, axis=0)**2)\n",
    "        noise_B = sigma * np.matmul(diff_B, D_B) * delta_B_B\n",
    "\n",
    "        D_C = np.diag(np.linalg.norm(diff_C, axis=0)**2)\n",
    "        noise_C = sigma * np.matmul(diff_C, D_C) * delta_B_C\n",
    "\n",
    "        # Additional correction term applied column-wise\n",
    "        correction_A = -0.5 * dt * sigma**2 * (np.linalg.norm(diff_A, axis=0, keepdims=True)**2 * A)\n",
    "        correction_B = -0.5 * dt * sigma**2 * (np.linalg.norm(diff_B, axis=0, keepdims=True)**2 * B)\n",
    "        correction_C = -0.5 * dt * sigma**2 * (np.linalg.norm(diff_C, axis=0, keepdims=True)**2 * C)\n",
    "\n",
    "        # Update particle's A, B, and C matrices\n",
    "        particle['A'] += drift_A + noise_A + correction_A\n",
    "        particle['B'] += drift_B + noise_B + correction_B\n",
    "        particle['C'] += drift_C + noise_C + correction_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"# List to store reconstruction errors of the consensus point at each iteration\n",
    "consensus_errors = []\n",
    "\n",
    "# Main iteration loop\n",
    "for iteration in range(n_iterations):\n",
    "    # Step 1: Calculate the consensus point\n",
    "    consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "\n",
    "    # Step 2: Compute the reconstruction error for the consensus point\n",
    "    consensus_tensor = cp_to_tensor((np.ones(rank), [consensus_point['A'], consensus_point['B'], consensus_point['C']]))\n",
    "    consensus_error = tl.norm(tensor - consensus_tensor)\n",
    "\n",
    "    # Print the reconstruction error of the consensus point\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations}, Consensus Reconstruction Error: {consensus_error}\")\n",
    "\n",
    "    # Store the error for later plotting\n",
    "    consensus_errors.append(consensus_error)\n",
    "\n",
    "    # Step 3: Update the particles\n",
    "    isotropic_update(particles, consensus_point, lambda_, sigma, dt)\n",
    "    # or use anisotropic_update(particles, consensus_point, lambda_, sigma, dt) if desired\n",
    "\n",
    "# Final output: You could also save or analyze the final consensus point\n",
    "final_consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "final_consensus_tensor = cp_to_tensor((np.ones(rank), [final_consensus_point['A'], final_consensus_point['B'], final_consensus_point['C']]))\n",
    "final_error = tl.norm(tensor - final_consensus_tensor)\n",
    "\n",
    "print(\"Final Consensus Reconstruction Error: \", final_error)\n",
    "\n",
    "# Plotting the consensus reconstruction errors over iterations\n",
    "plt.plot(consensus_errors, label='Consensus Reconstruction Error')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error of Consensus Point Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 20/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 21/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 22/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 23/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 24/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 25/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 26/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 27/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 28/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 29/100, Consensus Reconstruction Error: 2.9089886144862174\n",
      "Iteration 30/100, Consensus Reconstruction Error: 2.9089886144862174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     consensus_errors\u001b[38;5;241m.\u001b[39mappend(consensus_error)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Step 3: Update the particles (either isotropic or anisotropic)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[43misotropic_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsensus_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Collect errors at specific iterations: 1st, 1/4th, 1/2th, 3/4th, and last iteration\u001b[39;00m\n\u001b[1;32m     44\u001b[0m error_begin \u001b[38;5;241m=\u001b[39m consensus_errors[\u001b[38;5;241m0\u001b[39m]\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36misotropic_update\u001b[0;34m(particles, consensus_point, lambda_, sigma, dt)\u001b[0m\n\u001b[1;32m     15\u001b[0m A, B, C \u001b[38;5;241m=\u001b[39m particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m], particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m], particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m A_consensus, B_consensus, C_consensus \u001b[38;5;241m=\u001b[39m consensus_point[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m], consensus_point[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m], consensus_point[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m objective_function(\u001b[43mcompute_consensus_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m<\u001b[39m objective_function(particle):\n\u001b[1;32m     19\u001b[0m     drift_A \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mlambda_) \u001b[38;5;241m*\u001b[39m (A \u001b[38;5;241m-\u001b[39m A_consensus) \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m     20\u001b[0m     drift_B \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mlambda_) \u001b[38;5;241m*\u001b[39m (B \u001b[38;5;241m-\u001b[39m B_consensus) \u001b[38;5;241m*\u001b[39m dt\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mcompute_consensus_point\u001b[0;34m(particles, alpha)\u001b[0m\n\u001b[1;32m     13\u001b[0m A, B, C \u001b[38;5;241m=\u001b[39m particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m], particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m], particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute the energy for the current particle\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m energy \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply the numerical stabilization trick\u001b[39;00m\n\u001b[1;32m     19\u001b[0m weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39malpha \u001b[38;5;241m*\u001b[39m (energy \u001b[38;5;241m-\u001b[39m min_energy))\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(particle)\u001b[0m\n\u001b[1;32m      4\u001b[0m B \u001b[38;5;241m=\u001b[39m particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m C \u001b[38;5;241m=\u001b[39m particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m reconstructed_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcp_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Compute the Frobenius norm of the difference\u001b[39;00m\n\u001b[1;32m      9\u001b[0m error \u001b[38;5;241m=\u001b[39m tl\u001b[38;5;241m.\u001b[39mnorm(tensor \u001b[38;5;241m-\u001b[39m reconstructed_tensor)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorly/cp_tensor.py:486\u001b[0m, in \u001b[0;36mcp_to_tensor\u001b[0;34m(cp_tensor, mask)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     full_tensor \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    483\u001b[0m         khatri_rao([factors[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m weights] \u001b[38;5;241m+\u001b[39m factors[\u001b[38;5;241m1\u001b[39m:], mask\u001b[38;5;241m=\u001b[39mmask), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    484\u001b[0m     )\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorly/base.py:79\u001b[0m, in \u001b[0;36mfold\u001b[0;34m(unfolded_tensor, mode, shape)\u001b[0m\n\u001b[1;32m     77\u001b[0m mode_dim \u001b[38;5;241m=\u001b[39m full_shape\u001b[38;5;241m.\u001b[39mpop(mode)\n\u001b[1;32m     78\u001b[0m full_shape\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, mode_dim)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoveaxis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43munfolded_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorly/backend/__init__.py:206\u001b[0m, in \u001b[0;36mBackendManager.dispatch_backend_method.<locals>.wrapped_backend_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_backend_method\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;124;03m\"\"\"A dynamically dispatched method\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Returns the queried method from the currently set backend\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_THREAD_LOCAL_DATA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmoveaxis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:1455\u001b[0m, in \u001b[0;36mmoveaxis\u001b[0;34m(a, source, destination)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     transpose \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose\n\u001b[1;32m   1454\u001b[0m source \u001b[38;5;241m=\u001b[39m normalize_axis_tuple(source, a\u001b[38;5;241m.\u001b[39mndim, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1455\u001b[0m destination \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_axis_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdestination\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(source) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(destination):\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`source` and `destination` arguments must have \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1458\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe same number of elements\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:1381\u001b[0m, in \u001b[0;36mnormalize_axis_tuple\u001b[0;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(axis) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1381\u001b[0m         axis \u001b[38;5;241m=\u001b[39m [\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m(axis)]\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1383\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Create itertools.product to generate all combinations of parameters\n",
    "param_grid = list(itertools.product(dt, n_particles, alpha, lambda_, sigma))\n",
    "\n",
    "# Iterate over the parameter grid\n",
    "for params in param_grid:\n",
    "    dt, n_particles, alpha, lambda_, sigma = params\n",
    "\n",
    "    n_iterations = int(T / dt) # Varying parameter: Number of iterations\n",
    "\n",
    "    # Step 2: Initialize\n",
    "    particles = []\n",
    "    for _ in range(n_particles):\n",
    "        A = np.random.randn(I, rank) # n_particles Ixrank matrices\n",
    "        B = np.random.randn(J, rank) # n_particles Jxrank matrices\n",
    "        C = np.random.randn(K, rank) # n_particles Kxrank matrices\n",
    "        particles.append({'A': A, 'B': B, 'C': C})\n",
    "\n",
    "    # Print all parameters including fixed ones\n",
    "    print(f\"Running grid search with params: dt={dt}, n_particles={n_particles}, alpha={alpha}, lambda_={lambda_}, sigma={sigma}, T={T}, I={I}, J={J}, K={K}, rank={rank}, #iters={n_iterations}\")\n",
    "\n",
    "    # List to store reconstruction errors of the consensus point at each iteration\n",
    "    consensus_errors = []\n",
    "\n",
    "    # Main iteration loop\n",
    "    for iteration in range(n_iterations):\n",
    "        # Step 1: Calculate the consensus point\n",
    "        consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "\n",
    "        # Step 2: Compute the reconstruction error for the consensus point\n",
    "        consensus_tensor = cp_to_tensor((np.ones(rank), [consensus_point['A'], consensus_point['B'], consensus_point['C']]))\n",
    "        consensus_error = tl.norm(tensor - consensus_tensor)\n",
    "\n",
    "        # Print the reconstruction error of the consensus point\n",
    "        print(f\"Iteration {iteration + 1}/{n_iterations}, Consensus Reconstruction Error: {consensus_error}\")\n",
    "\n",
    "        # Store the error for later plotting\n",
    "        consensus_errors.append(consensus_error)\n",
    "\n",
    "        # Step 3: Update the particles (either isotropic or anisotropic)\n",
    "        isotropic_update(particles, consensus_point, lambda_, sigma, dt)\n",
    "\n",
    "    # Collect errors at specific iterations: 1st, 1/4th, 1/2th, 3/4th, and last iteration\n",
    "    error_begin = consensus_errors[0]\n",
    "    error_iter_1_4 = consensus_errors[int(n_iterations * 0.25)]\n",
    "    error_iter_1_2 = consensus_errors[int(n_iterations * 0.5)]\n",
    "    error_iter_3_4 = consensus_errors[int(n_iterations * 0.75)]\n",
    "    error_last = consensus_errors[-1]\n",
    "\n",
    "    # Add the parameters and errors to the results list\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'lambda': lambda_,\n",
    "        'sigma': sigma,\n",
    "        'T': T,\n",
    "        'dt': dt,\n",
    "        '#iters (T/dt)': n_iterations,\n",
    "        '#particles': n_particles,\n",
    "        'I': I,\n",
    "        'J': J,\n",
    "        'K': K,\n",
    "        'rank': rank,\n",
    "        'error_begin': error_begin,\n",
    "        'error_iter 1/4': error_iter_1_4,\n",
    "        'error_iter 1/2': error_iter_1_2,\n",
    "        'error_iter 3/4': error_iter_3_4,\n",
    "        'error_iter last': error_last,\n",
    "        'landscape': ''  # Fill this field if needed\n",
    "    })\n",
    "\n",
    "    # Final output: Save or analyze the final consensus point\n",
    "    final_consensus_point = compute_consensus_point(particles, alpha=alpha)\n",
    "    final_consensus_tensor = cp_to_tensor((np.ones(rank), [final_consensus_point['A'], final_consensus_point['B'], final_consensus_point['C']]))\n",
    "    final_error = tl.norm(tensor - final_consensus_tensor)\n",
    "\n",
    "    print(\"Final Consensus Reconstruction Error: \", final_error)\n",
    "\n",
    "    # Plotting the consensus reconstruction errors over iterations\n",
    "    plt.plot(consensus_errors, label=f'Consensus Error (dt={dt}, np={n_particles}, α={alpha}, λ={lambda_}, σ={sigma})')\n",
    "\n",
    "# Final plot formatting (if multiple runs are plotted)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error of Consensus Point Over Iterations (Grid Search)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
